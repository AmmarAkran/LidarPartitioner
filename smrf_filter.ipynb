{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06384b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02898821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import ceil\n",
    "import yaml, laspy\n",
    "import lithops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neilpy\n",
    "from lithops.storage.cloud_proxy import open, os as os_cl\n",
    "from lithops import ServerlessExecutor\n",
    "from lidarpartitioner import lidarutils, utils\n",
    "from lidarpartitioner.las_partitioner import Partitioner\n",
    "from lidarpartitioner.lidarutils import *\n",
    "import lidarpartitioner\n",
    "from lithops.worker.utils import get_memory_usage\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "datet = dt_string.replace(\":\", \"_\").replace(\" \",\"_\").replace(\"/\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48224d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_file(fname, out_dir, num_points, sufix, buffer):\n",
    "    divider = Partitioner(fname, sufix)\n",
    "    m_threshold = num_points\n",
    "    divider.make_partition(out_dir, capacity=m_threshold, buffer=buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smrf_filter(id, obj, args, ibm_cos):\n",
    "    \n",
    "    st_map_process = time.time()\n",
    "    print(args)\n",
    "    print(round(st_map_process - args['st_call'], 3))\n",
    "    print(id)\n",
    "    print(obj.key)\n",
    "    Allprocessing_time = dict()\n",
    "    Allprocessing_time['Object_name'] = obj.key\n",
    "    Allprocessing_time['Object_ID'] = id\n",
    "    Allprocessing_time['PeriodTime_Mapstart'] = round(st_map_process - args['st_call'], 3)\n",
    "    Allprocessing_time['res_bucket'] = args['res_bucket']\n",
    "    \n",
    "    # Create folders\n",
    "    ordir = '/tiles'\n",
    "    ldr_utls.rem_folder(ordir)\n",
    "    fname = os.path.join(ordir, obj.key)\n",
    "    \n",
    "    \n",
    "    # read data stream\n",
    "    st_read = time.time()\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(obj.data_stream.read())\n",
    "        \n",
    "    read_data = time.time() - st_read\n",
    "    Allprocessing_time['RWData_time'] = round(read_data, 3)    \n",
    "       \n",
    "    \n",
    "    # Run SMRF\n",
    "    st_filter = time.time()\n",
    "    cellsize = 2\n",
    "    windows = 12\n",
    "    slope_threshold = .15 \n",
    "    elevation_threshold = 0.30\n",
    "    elevation_scaler = 0.95\n",
    "    \n",
    "    \n",
    "    inF = laspy.file.File(fname, mode='r')                  # Read points\n",
    "    xyz = np.vstack((inF.x, inF.y, inF.z, inF.withheld)).T\n",
    "    df = pd.DataFrame(data=xyz, columns=['X', 'Y', 'Z', 'Withheld'])\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        Zsmrf, Tsmrf, obj_cells, obj_points = neilpy.smrf(df.X,df.Y,df.Z,df.Withheld,cellsize,windows,slope_threshold,\n",
    "                                                      elevation_threshold,elevation_scaler)\n",
    "        \n",
    "#         Zsmrf, Tsmrf, obj_cells, obj_points = neilpy.smrf(df.X,df.Y,df.Z,cellsize,windows,slope_threshold,\n",
    "#                                                           elevation_threshold,elevation_scaler)\n",
    "        \n",
    "        filtering_time = time.time() - st_filter\n",
    "            \n",
    "        # Save the ground points \n",
    "        st_wres = time.time()\n",
    "        fname, data = lidarutils.writer_lasfile(inF, obj_points, fname, args['res_bucket'], \n",
    "                                                 reduce_stream = True)\n",
    "        inF.close()\n",
    "        en_wres = time.time() - st_wres\n",
    "    except Exception as e:\n",
    "        print('Failed!!. Reason: %s' %(e))\n",
    "        \n",
    "    maping_time = time.time() - st_map_process\n",
    "    Allprocessing_time['Filtering_time'] = round(filtering_time, 3)\n",
    "    Allprocessing_time['Save_BE_points'] = round(en_wres, 3)\n",
    "    Allprocessing_time['Maping_time'] = round(time.time() - st_map_process, 3)\n",
    "    Allprocessing_time['sendResult_time'] = time.time()\n",
    "    \n",
    "    return Allprocessing_time, data\n",
    "\n",
    "def merg_res(results):\n",
    "    st_redu = time.time()\n",
    "    pr_time = dict()\n",
    "    merging = dict()\n",
    "    Filtering_time, RWData_time, Maping_time, sendResult_time  = [], [], [], []\n",
    "    Save_BE_points, PeriodTime_Mapstart = [], []\n",
    "    num_part = len(results)\n",
    "    print(num_part)\n",
    "    out_bucket = 'finalresult'\n",
    "    in_bucket = results[0][0]['res_bucket']\n",
    "    \n",
    "    res_mapdata = []\n",
    "    for res in results:\n",
    "        obj = dict()\n",
    "        RWData_time.append(res[0]['RWData_time'])\n",
    "        Filtering_time.append(res[0]['Filtering_time'])\n",
    "        Save_BE_points.append(res[0]['Save_BE_points'])\n",
    "        Maping_time.append(res[0]['Maping_time'])\n",
    "        PeriodTime_Mapstart.append(res[0]['PeriodTime_Mapstart'])\n",
    "        sendResult_time.append(st_redu - res[0]['sendResult_time'])\n",
    "        obj['Object_name'] = res[0]['Object_name']\n",
    "        obj['data_stream'] = res[1]\n",
    "        res_mapdata.append(obj)\n",
    "    del results\n",
    "    \n",
    "    # Start merging all the results\n",
    "    st_merg = time.time()\n",
    "    m_res = ldr_utls.byt_merg_streamres(res_mapdata, out_bucket, num_part)\n",
    "    merging_time = time.time() - st_merg\n",
    "#     merging['Merging_time'] = merging_time\n",
    "    \n",
    "    pr_time['Funcs_numbers'] = num_part\n",
    "    pr_time['File_name'] = m_res['File_name']\n",
    "    pr_time['Points_number'] = m_res['Points_number']\n",
    "    pr_time['PeriodTime_Mapstart'] = min(PeriodTime_Mapstart)\n",
    "    pr_time['PeriodTime_redstart'] = round(min(sendResult_time), 3)\n",
    "    pr_time['DSData_time'] = max(RWData_time)\n",
    "    pr_time['Filtering_time'] = max(Filtering_time)\n",
    "    pr_time['Save_BE_points'] = max(Save_BE_points)\n",
    "    pr_time['Maping_time'] = max(Maping_time)\n",
    "    pr_time['Saving_data_time'] = m_res['Saving_data_time']\n",
    "    pr_time['Merging_files_time'] = m_res['Merging_files_time']\n",
    "    pr_time['Merged_uploading_time'] = m_res['Merged_uploading_time']\n",
    "    pr_time['Merging_process'] = m_res['Merging_process']\n",
    "    pr_time['Reduction_time'] = round(time.time() - st_redu, 3)\n",
    "    \n",
    "    return pr_time, time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181713e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "processing_time = []\n",
    "data_folder = 'data'\n",
    "chip_folder = 'chipres'\n",
    "res_map_folder = 'removed_outliers'\n",
    "res_red_folder = 'merged_files'\n",
    "fname = data_folder + '/' + \"lasfile.las\"\n",
    "#\"Coloreado (RGB) 2016 - PNOA-2016-CAT-352-4556-ORT-CLA-COL.las\" \"lasfile.las\"\n",
    "# \"Coloreado (RGB) 2016 - PNOA-2016-CAT-352-4554-ORT-CLA-COL.las\"\n",
    "# \"Coloreado (RGB) 2016 - PNOA-2016-CAT-338-4556-ORT-CLA-COL.las\"\n",
    "folders = [chip_folder, res_map_folder]\n",
    "\n",
    "for i in range(2, 3):\n",
    "    \n",
    "    \n",
    "    if i > 0:\n",
    "        \n",
    "        # Start partitioning\n",
    "        # Clean folder in bucket if exists, if not create it\n",
    "        [lidarutils.clean_folder(folder) for folder in folders]\n",
    "    \n",
    "        # Set variables and start partitioning\n",
    "        partitions = 2**i\n",
    "        buffer = 0\n",
    "        num_points = ceil(343306/partitions) #5492898 3791954 1373224 343306\n",
    "        print(num_points)\n",
    "        \n",
    "        partition_args = [(fname, chip_folder, num_points, i, buffer) for i in range(1)]\n",
    "        st_split = time.time()\n",
    "        with ServerlessExecutor(runtime_memory=3072) as exec: #runtime_memory=3072\n",
    "            exec.map(partition_file, partition_args)\n",
    "            exec.get_result()\n",
    "        end_split = time.time() - st_split\n",
    "        print(f'Total partition time: {end_split} s')\n",
    "    \n",
    "    # Processing files resulting from the partition stage \n",
    "    st_processing = time.time()\n",
    "    if i == 0:\n",
    "        map_runtime_memory = 2048\n",
    "    elif i > 4:\n",
    "        map_runtime_memory = 512\n",
    "    else:\n",
    "        map_runtime_memory = 1024\n",
    "        \n",
    "    files = os_cl.listdir(chip_folder)\n",
    "    kwargs = (map_iterdata, {'res_bucket': res_bucket, 'st_call': time.time()}) if i > 0 else (split_iterdata, {'res_bucket': res_bucket, 'st_call': time.time()})\n",
    "    with ServerlessExecutor(runtime=\"ammarokran/new-smrf-conda36:1.0.6\") as exec:\n",
    "        exec.map_reduce(smrf_filter, kwargs, merg_res, map_runtime_memory=map_runtime_memory,\n",
    "                        include_modules=['lidarutils', 'lidarpartitioner', 'utils'])\n",
    "        st_get = time.time()\n",
    "        res = exec.get_result()\n",
    "        end_get = time.time() - st_get\n",
    "        # fexec.clean()\n",
    "    end_processing = time.time() - st_processing\n",
    "    res['GettingResult_time'] = round(end_get, 3)\n",
    "    res['Processing_time'] = round(end_processing, 3)\n",
    "    processing_time.append(res)\n",
    "    print(f'Total processing time: {end_processing} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpeedUp \n",
    "par_times = []\n",
    "df = pd.DataFrame(processing_time)\n",
    "seq_time = df.iloc[0]['Processing_time']\n",
    "\n",
    "par_times = [df.iloc[i]['Processing_time'] for i in range(1, len(df))]\n",
    "SpUp = [str(round((seq_time/v), 5))+'x' for v in par_times]\n",
    "SpUp.insert(0, str(0) + ' (base)')\n",
    "# # print(seq_time, diff)\n",
    "print('SpeedUp %s' % (SpUp))\n",
    "df_spup = pd.DataFrame(SpUp, columns=['SpeedUp'])\n",
    "# df_spup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spup['Funcs_num'] = df['Funcs_numbers']\n",
    "df_spup['Processing_time'] = df['Processing_time']\n",
    "df_spup = df_spup[['Funcs_num', 'Processing_time', 'SpeedUp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffe8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(processing_time)\n",
    "A = list(df2['Processing_time'])\n",
    "\n",
    "ind = np.arange(len(A))    # the x locations for the groups\n",
    "width = 0.7       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, A, edgecolor='white', width=width)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.5, ls='--', zorder=0)\n",
    "plt.xlabel('Number of Functions')\n",
    "plt.ylabel('Time by Second (s)')\n",
    "# plt.title('The processing time by using Lithops framework')\n",
    "plt.xticks(ind, list(df2['Funcs_numbers']))\n",
    "plt.savefig('./figures_outlier/Processing Time.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Proc_time'] = df2['Filtering_time'] + df2['Save_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7974364",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = list(df2['Downloading_Time'])\n",
    "B = list(df2['Proc_time'])\n",
    "C = list(df2['Time_Redstart'])\n",
    "D = list(df2['Reduction_time'])\n",
    "# E = list(df2['Reduction_time'])\n",
    "\n",
    "bar1 = np.add(A, B).tolist()\n",
    "bar2 = np.add(A, np.add(B, C)).tolist()\n",
    "# bar3 = np.add(np.add(A, B), np.add(C, D)).tolist()\n",
    "ind = np.arange(len(B)) \n",
    "width = 0.7\n",
    "\n",
    "p1 = plt.bar(ind, A, edgecolor='white', width=width, color='purple')\n",
    "p2 = plt.bar(ind, B, edgecolor='white', width=width,\n",
    "             bottom=A)\n",
    "p3 = plt.bar(ind, C, edgecolor='white', width=width,\n",
    "             bottom=bar1, color='grey')\n",
    "p4 = plt.bar(ind, D, edgecolor='white', width=width,\n",
    "             bottom=bar2)\n",
    "# p5 = plt.bar(ind, E, edgecolor='white', width=width,\n",
    "#              bottom=bar3)\n",
    "plt.grid(axis='y', alpha=0.5, ls='--', zorder=0)\n",
    "plt.xlabel('Number of Functions')\n",
    "plt.ylabel('Time by Second (s)')\n",
    "# plt.title('Approach #1: Time of Processing at a time %s'% (dt_string))\n",
    "plt.xticks(ind, list(df2['Funcs_numbers']))\n",
    "plt.legend((p1[0], p2[0], p3[0], p4[0]), ('Downloading Time(C1)', 'Processing Time(C2)', 'Obtaining Results Time(C3)', \n",
    "                                   'Reduction Time(C4)'), loc='upper left', bbox_to_anchor=(0.41, 1.02))\n",
    "plt.savefig('./figures_outlier/Processing Time plot at time %s.png'% (datet), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
