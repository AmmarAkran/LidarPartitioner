{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "# from cos_backend import CosBackend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from math import ceil\n",
    "import yaml, laspy\n",
    "import lithops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neilpy\n",
    "from lithops.storage.cloud_proxy import open, os as os_cl\n",
    "from lithops import ServerlessExecutor\n",
    "from lidarpartitioner import lidarutils, utils\n",
    "from lidarpartitioner.las_partitioner import Partitioner\n",
    "from lidarpartitioner.lidarutils import *\n",
    "import lidarpartitioner\n",
    "from lithops.worker.utils import get_memory_usage\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea241b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "datet = dt_string.replace(\":\", \"_\").replace(\" \",\"_\").replace(\"/\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ae6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_file(fname, out_dir, num_points, sufix, buffer):\n",
    "    divider = Partitioner(fname, sufix)\n",
    "    m_threshold = num_points\n",
    "    divider.make_partition(out_dir, capacity=m_threshold, buffer=buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1fc3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_filter(id, file, smr_folder, buffer, call_time):\n",
    "    \n",
    "    st_map_process = time.time()\n",
    "    Allprocessing_time = dict()\n",
    "    Allprocessing_time['Object_name'] = file.split('/')[-1]\n",
    "    Allprocessing_time['Map_ID'] = id\n",
    "    Allprocessing_time['Buffer'] = buffer\n",
    "    Allprocessing_time['Time_Mapstart'] = round(st_map_process - call_time, 3)\n",
    "    print(\"ID of the function: %s\" % str(id))\n",
    "    \n",
    "    \n",
    "    # read data stream\n",
    "    st_read = time.time()\n",
    "    with open(file, 'rb') as f:\n",
    "        inF = laspy.read(f)\n",
    "    point_cloud = np.vstack((inF.x, inF.y, inF.z, inF.withheld)).T\n",
    "    download_data = time.time() - st_read\n",
    "    Allprocessing_time['Downloading_Time'] = round(download_data, 3) \n",
    "    \n",
    "    \n",
    "    # Start applying outliers filter (radius method)\n",
    "    # https://pdal.io/PDAL.pdf\n",
    "    st_filter = time.time()\n",
    "    radius = 2\n",
    "    min_k = 4\n",
    "    out_bucket = 'smrfresult'\n",
    "    print(len(point_cloud))\n",
    "    try:\n",
    "        tree = KDTree(point_cloud[:,:3], leaf_size=40)\n",
    "        b = point_cloud[:,3] == 0\n",
    "        print(set(b), len(b))\n",
    "        p_ind = np.where(b)\n",
    "        k = tree.query_radius(point_cloud[b][:,:3], r=radius, count_only=True)\n",
    "        outlier = np.asarray(list(map(lambda x: x < min_k, k)))\n",
    "        not_outlier = list(~np.array(outlier))\n",
    "        indices = np.asarray(p_ind[0])[not_outlier]\n",
    "        filtering_time = time.time() - st_filter\n",
    "        \n",
    "        st_wres = time.time()\n",
    "        fname, data = lidarutils.writer_lasfile(inF, indices, file, out_bucket, out_dir=smr_folder,\n",
    "                                                 reduce_stream = True, is_bool = False)\n",
    "        en_wres = time.time() - st_wres\n",
    "    except Exception as e:\n",
    "        print('Failed!!. Reason: %s' %(e))\n",
    "    \n",
    "    Allprocessing_time['Filtering_time'] = round(filtering_time, 3)\n",
    "    Allprocessing_time['Save_points'] = round(en_wres, 3)\n",
    "    Allprocessing_time['Maping_time'] = round(time.time() - st_map_process, 3)\n",
    "    Allprocessing_time['sendResult_time'] = time.time()\n",
    "    \n",
    "    return Allprocessing_time, data\n",
    "\n",
    "\n",
    "def merg_res(results):\n",
    "\n",
    "    st_reducer = time.time()\n",
    "    pr_time = dict()\n",
    "    merging = dict()\n",
    "    Filtering_time, RWData_time, Maping_time, sendResult_time  = [], [], [], []\n",
    "    Save_points, PeriodTime_Mapstart = [], []\n",
    "    num_part = len(results)\n",
    "    buffer = results[0][0]['Buffer']\n",
    "    print(num_part)\n",
    "    \n",
    "    res_mapdata = []\n",
    "    for res in results:\n",
    "        obj = dict()\n",
    "        RWData_time.append(res[0]['Downloading_Time'])\n",
    "        Filtering_time.append(res[0]['Filtering_time'])\n",
    "        Save_points.append(res[0]['Save_points'])\n",
    "        Maping_time.append(res[0]['Maping_time'])\n",
    "        PeriodTime_Mapstart.append(res[0]['Time_Mapstart'])\n",
    "        sendResult_time.append(st_reducer - res[0]['sendResult_time'])\n",
    "        obj['Object_name'] = res[0]['Object_name']\n",
    "        obj['data_stream'] = res[1]\n",
    "        res_mapdata.append(obj)\n",
    "    del results\n",
    "    \n",
    "    # Start merging all the results\n",
    "    st_merg = time.time()\n",
    "    m_res = merg_files(res_mapdata, num_part, buffer)\n",
    "    \n",
    "    merging_time = time.time() - st_merg\n",
    "    merging['Merging_time'] = merging_time\n",
    "    \n",
    "    pr_time['Funcs_numbers'] = num_part\n",
    "    pr_time['File_name'] = m_res['File_name']\n",
    "    pr_time['Points_number'] = m_res['Points_number']\n",
    "    pr_time['Time_Mapstart'] = min(PeriodTime_Mapstart)\n",
    "    pr_time['Time_Redstart'] = round(min(sendResult_time), 3)\n",
    "    pr_time['Downloading_Time'] = max(RWData_time)\n",
    "    pr_time['Filtering_time'] = max(Filtering_time)\n",
    "    pr_time['Save_points'] = max(Save_points)\n",
    "    pr_time['Maping_time'] = max(Maping_time)\n",
    "    pr_time['Saving_data_time'] = m_res['Saving_data_time']\n",
    "    pr_time['Merging_files_time'] = m_res['Merging_files_time']\n",
    "    pr_time['Merged_uploading_time'] = m_res['Merged_uploading_time']\n",
    "    pr_time['Merging_process'] = m_res['Merging_process']\n",
    "    pr_time['Reduction_time'] = round(time.time() - st_reducer, 3)\n",
    "    \n",
    "    return pr_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57284dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processing_time = []\n",
    "data_folder = 'data'\n",
    "chip_folder = 'chipres'\n",
    "res_map_folder = 'removed_outliers'\n",
    "res_red_folder = 'merged_files'\n",
    "fname = data_folder + '/' + \"lasfile.las\"\n",
    "#\"Coloreado (RGB) 2016 - PNOA-2016-CAT-352-4556-ORT-CLA-COL.las\" \"lasfile.las\"\n",
    "# \"Coloreado (RGB) 2016 - PNOA-2016-CAT-352-4554-ORT-CLA-COL.las\"\n",
    "# \"Coloreado (RGB) 2016 - PNOA-2016-CAT-338-4556-ORT-CLA-COL.las\"\n",
    "folders = [chip_folder, res_map_folder]\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    \n",
    "    if i > 0:\n",
    "        \n",
    "        # Start partitioning\n",
    "        # Clean folder in bucket if exists, if not create it\n",
    "        [lidarutils.clean_folder(folder) for folder in folders]\n",
    "    \n",
    "        # Set variables and start partitioning\n",
    "        partitions = 2**i\n",
    "        buffer = 0\n",
    "        num_points = ceil(343306/partitions) #5492898 3791954 1373224 343306\n",
    "        print(num_points)\n",
    "        \n",
    "        partition_args = [(fname, chip_folder, num_points, i, buffer) for i in range(1)]\n",
    "        st_split = time.time()\n",
    "        with ServerlessExecutor(runtime_memory=3072) as exec: #runtime_memory=3072\n",
    "            exec.map(partition_file, partition_args)\n",
    "            exec.get_result()\n",
    "        end_split = time.time() - st_split\n",
    "        print(f'Total partition time: {end_split} s')\n",
    "    \n",
    "    # Processing files resulting from the partition stage \n",
    "    st_processing = time.time()\n",
    "    if i == 0:\n",
    "        map_runtime_memory = 2048\n",
    "    elif i > 4:\n",
    "        map_runtime_memory = 512\n",
    "    else:\n",
    "        map_runtime_memory = 1024\n",
    "        \n",
    "    files = os_cl.listdir(chip_folder)\n",
    "    process_args = [(chip_folder + '/' + file, res_map_folder, buffer, st_processing) for file in files] if i > 0 \\\n",
    "                    else [(fname, res_map_folder, 0, st_processing)]\n",
    "    with ServerlessExecutor(runtime=\"ammarokran/new-smrf-conda36:1.0.6\") as exec:\n",
    "        exec.map_reduce(outlier_filter, process_args, merg_res, map_runtime_memory=map_runtime_memory,\n",
    "                        include_modules=['lidarutils', 'lidarpartitioner', 'utils'])\n",
    "        st_get = time.time()\n",
    "        res = exec.get_result()\n",
    "        end_get = time.time() - st_get\n",
    "        # fexec.clean()\n",
    "    end_processing = time.time() - st_processing\n",
    "    res['GettingResult_time'] = round(end_get, 3)\n",
    "    res['Processing_time'] = round(end_processing, 3)\n",
    "    processing_time.append(res)\n",
    "    print(f'Total processing time: {end_processing} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpeedUp \n",
    "par_times = []\n",
    "df = pd.DataFrame(processing_time)\n",
    "seq_time = df.iloc[0]['Processing_time']\n",
    "\n",
    "par_times = [df.iloc[i]['Processing_time'] for i in range(1, len(df))]\n",
    "SpUp = [str(round((seq_time/v), 5))+'x' for v in par_times]\n",
    "SpUp.insert(0, str(0) + ' (base)')\n",
    "# # print(seq_time, diff)\n",
    "print('SpeedUp %s' % (SpUp))\n",
    "df_spup = pd.DataFrame(SpUp, columns=['SpeedUp'])\n",
    "# df_spup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spup['Funcs_num'] = df['Funcs_numbers']\n",
    "df_spup['Processing_time'] = df['Processing_time']\n",
    "df_spup = df_spup[['Funcs_num', 'Processing_time', 'SpeedUp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(processing_time)\n",
    "A = list(df2['Processing_time'])\n",
    "\n",
    "ind = np.arange(len(A))    # the x locations for the groups\n",
    "width = 0.7       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, A, edgecolor='white', width=width)\n",
    "\n",
    "plt.grid(axis='y', alpha=0.5, ls='--', zorder=0)\n",
    "plt.xlabel('Number of Functions')\n",
    "plt.ylabel('Time by Second (s)')\n",
    "# plt.title('The processing time by using Lithops framework')\n",
    "plt.xticks(ind, list(df2['Funcs_numbers']))\n",
    "plt.savefig('./figures_outlier/Processing Time.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b45821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Proc_time'] = df2['Filtering_time'] + df2['Save_points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c88ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = list(df2['Downloading_Time'])\n",
    "B = list(df2['Proc_time'])\n",
    "C = list(df2['Time_Redstart'])\n",
    "D = list(df2['Reduction_time'])\n",
    "# E = list(df2['Reduction_time'])\n",
    "\n",
    "bar1 = np.add(A, B).tolist()\n",
    "bar2 = np.add(A, np.add(B, C)).tolist()\n",
    "# bar3 = np.add(np.add(A, B), np.add(C, D)).tolist()\n",
    "ind = np.arange(len(B)) \n",
    "width = 0.7\n",
    "\n",
    "p1 = plt.bar(ind, A, edgecolor='white', width=width, color='purple')\n",
    "p2 = plt.bar(ind, B, edgecolor='white', width=width,\n",
    "             bottom=A)\n",
    "p3 = plt.bar(ind, C, edgecolor='white', width=width,\n",
    "             bottom=bar1, color='grey')\n",
    "p4 = plt.bar(ind, D, edgecolor='white', width=width,\n",
    "             bottom=bar2)\n",
    "# p5 = plt.bar(ind, E, edgecolor='white', width=width,\n",
    "#              bottom=bar3)\n",
    "plt.grid(axis='y', alpha=0.5, ls='--', zorder=0)\n",
    "plt.xlabel('Number of Functions')\n",
    "plt.ylabel('Time by Second (s)')\n",
    "# plt.title('Approach #1: Time of Processing at a time %s'% (dt_string))\n",
    "plt.xticks(ind, list(df2['Funcs_numbers']))\n",
    "plt.legend((p1[0], p2[0], p3[0], p4[0]), ('Downloading Time(C1)', 'Processing Time(C2)', 'Obtaining Results Time(C3)', \n",
    "                                   'Reduction Time(C4)'), loc='upper left', bbox_to_anchor=(0.41, 1.02))\n",
    "plt.savefig('./figures_outlier/Processing Time plot at time %s.png'% (datet), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df98fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
